{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14597,"status":"ok","timestamp":1645940485310,"user":{"displayName":"Chua Cheng Hong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgB02yYr0OXz-1q3qlmnMPOfkLWcmGM_ckc4HHB=s64","userId":"15038074957184626047"},"user_tz":-480},"id":"E3nZjqKoqeYj","outputId":"3179fe9c-688f-4334-a97b-84d28882a08b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B1M2SbRTTwzR"},"outputs":[],"source":["%%capture\n","!pip install fvcore\n","# YOU WILL NEED TO GET YOUR OWN GITHUB CONFIG AND NGROK CONFIG\n","!git config --global include.path \"/content/drive/MyDrive/Colab Notebooks/.gitconfig\"\n","# connect to github\n","!gituser=\"$(git config --get user.name)\";\\\n","  gitpassword=\"$(git config --get user.password)\";\\\n","  git clone \"https://${gituser}:${gitpassword}@github.com/thecch/MDP.git\"\n","\n","!wget \"https://drive.google.com/u/0/uc?id=10zDK7ldKLbVwq0JA6gCDFvp1zewtAIJj&export=download&confirm=t\" -O /content/MDP/ObjectDetection/models/faster_rcnn_R_50_FPN_3x_final.pth\n","!wget \"https://drive.google.com/u/0/uc?id=1o50KO97_t8x-z9HUxKlK7RZUImcR66Dm&export=download&confirm=t\" -O /content/model_final.pt\n","!wget \"https://drive.google.com/u/0/uc?id=1T-yQDqHE74cHU-ipoDBgzsbQaNth1X9b&export=download&confirm=t\" -O /content/model_cpu.pt"]},{"cell_type":"markdown","metadata":{"id":"v4ubHGfZmMKS"},"source":["## REST API APP"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-5n_cUcywfs6"},"outputs":[],"source":["%%capture\n","!cp \"/content/drive/MyDrive/Colab Notebooks/ngrok.yml\" ~/.ngrok2/ngrok.yml\n","!pip install -q flask-ngrok\n","!pip install -q flask-bootstrap\n","!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","!unzip -o ngrok-stable-linux-amd64.zip\n","!./ngrok authtoken 25QCSWTSRQBBcpymPkeBlQRF8IB_82fNN35pEAp9yUVFA1RuR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7unkuuiqLdqd"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Basic setup:\n","import torch\n","import json\n","import numpy as np\n","import os, json, cv2, random\n","from datetime import datetime\n","\n","# api libraries\n","from flask_ngrok import run_with_ngrok\n","from flask import Flask, render_template, request, jsonify\n","\n","# Setup detectron2 logger\n","import detectron2\n","from detectron2.utils.logger import setup_logger\n","\n","# import common detectron2 utilities\n","from detectron2 import model_zoo\n","from detectron2.config import get_cfg\n","from detectron2.modeling import build_model\n","from detectron2.checkpoint import DetectionCheckpointer\n","from detectron2.engine import DefaultPredictor\n","\n","def build_inference_model():\n","  NUM_CLASS = 31\n","  MODEL = \"DETECTRON\"\n","  MODEL_NAME = \"faster_rcnn_R_50_FPN_3x\"\n","\n","  cfg = get_cfg()\n","  cfg.MODEL.DEVICE = 'cuda'\n","  cfg.OUTPUT_DIR = \"/content/MDP/ObjectDetection/models/output\"\n","  cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/\" + MODEL_NAME + \".yaml\"))\n","\n","  cfg.DATALOADER.NUM_WORKERS = 2\n","  cfg.MODEL.WEIGHTS = os.path.join('/content/MDP/ObjectDetection/models/', MODEL_NAME + '_final.pth')\n","  cfg.MODEL.RPN.PRE_NMS_TOPK_TRAIN = 200\n","  cfg.MODEL.RPN.PRE_NMS_TOPK_TEST = 100\n","  cfg.MODEL.RPN.POST_NMS_TOPK_TRAIN = 100\n","  cfg.MODEL.RPN.POST_NMS_TOPK_TEST = 100\n","\n","  cfg.MODEL.BACKBONE.FREEZE_AT = 2\n","\n","  cfg.SOLVER.IMS_PER_BATCH = 4\n","  cfg.SOLVER.BASE_LR = 0.005\n","  cfg.SOLVER.STEPS = (11000, 14000)\n","  cfg.SOLVER.MAX_ITER = 15000\n","  cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64\n","  cfg.MODEL.ROI_HEADS.NUM_CLASSES = NUM_CLASS\n","  cfg.TEST.DETECTIONS_PER_IMAGE = 3\n","\n","  model = build_model(cfg)\n","  DetectionCheckpointer(model).load(cfg.MODEL.WEIGHTS)\n","\n","  cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7\n","  return DefaultPredictor(cfg)\n","\n","\n","Predictor = build_inference_model()\n","mapper_json = json.load(open('/content/MDP/ObjectDetection/mapper.json'))['categories']\n","mapper = dict([(mapper['id'], mapper['name']) for mapper in mapper_json])\n","\n","def infer_img(img, pred = Predictor):\n","  outputs = pred(cv2.imread(img))\n","  return outputs[\"instances\"].to(\"cpu\")\n","\n","def process_result(resp, file_name):\n","  inference_results = []\n","  for i in range(len(resp)):\n","    inference_result = {\n","      'pred_classes': mapper[resp.pred_classes[i].item()],\n","      'confidence': resp.scores[i].item(),\n","      'x1': resp.pred_boxes[i].tensor[0][0].item(),\n","      'y1': resp.pred_boxes[i].tensor[0][1].item(),\n","      'x2': resp.pred_boxes[i].tensor[0][2].item(),\n","      'y2': resp.pred_boxes[i].tensor[0][3].item()\n","    }\n","    inference_results.append(inference_result)\n","\n","  print('File: {} \\nInference: {}'.format(file_name, inference_results))\n","  return jsonify(inference_results)\n","\n","\n","app = Flask(__name__)\n","run_with_ngrok(app)\n","\n","history = dict()\n","\n","@app.route(\"/\", methods = [\"GET\"])\n","def main():\n","  return \"\"\"\n","  Post image to extension /img_rec.</br></br>\n","\n","  Upload should be in this format. { 'image': open(img_path, 'rb') }</br></br>\n","\n","  Sample API call.</br></br>\n","\n","  </blockquote>\n","  import requests</br></br>\n","\n","  r = requests.post(</br>\n","    url + '/img_rec',</br>\n","    files = {</br>\n","    'image': open(img_path, 'rb')</br>\n","    }</br>\n","  ).json()</br>\n","  </blockquote>\n","  \"\"\"\n","\n","@app.route(\"/img_rec\", methods = [\"POST\", \"GET\"])\n","def img_rec():\n","  file_name = '/content/{}'.format(datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n","\n","  file = request.files['image']\n","  file.save(file_name + '.jpg')\n","\n","  resp = infer_img(file_name + '.jpg')\n","  history[file_name] = resp\n","\n","  return process_result(resp, file_name)\n","\n","\n","\n","app.run()"]},{"cell_type":"markdown","metadata":{"id":"ghFE7bu3wKk9"},"source":["## Export"]},{"cell_type":"markdown","source":["### Model"],"metadata":{"id":"zo2BzdDe2Wbl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"NeVOXlSdtvuf"},"outputs":[],"source":["%%capture\n","!pip install -q pyyaml==5.1\n","import torch\n","TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n","CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n","# Install detectron2 that matches the above pytorch version\n","!pip install -q detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/$CUDA_VERSION/torch$TORCH_VERSION/index.html\n","!pip install onnx==v1.8.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mBXhXWdLea2O"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Basic setup:\n","import torch\n","import numpy as np\n","import os, json, cv2, random\n","\n","# detectron\n","import detectron2.data.transforms as T\n","\n","# import common detectron2 utilities\n","from detectron2 import model_zoo\n","from detectron2.config import get_cfg\n","from detectron2.modeling import build_model\n","from detectron2.checkpoint import DetectionCheckpointer\n","from detectron2.export import Caffe2Tracer, TracingAdapter\n","\n","\n","NUM_CLASS = 31\n","MODEL = \"DETECTRON\"\n","MODEL_NAME = \"faster_rcnn_R_50_FPN_3x\"\n","\n","cfg = get_cfg()\n","cfg.MODEL.DEVICE = 'cpu'\n","cfg.OUTPUT_DIR = \"/content/MDP/ObjectDetection/models/output\"\n","cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/\" + MODEL_NAME + \".yaml\"))\n","\n","cfg.DATALOADER.NUM_WORKERS = 2\n","cfg.MODEL.WEIGHTS = os.path.join('/content/MDP/ObjectDetection/models/', MODEL_NAME + '_final.pth')\n","cfg.MODEL.RPN.PRE_NMS_TOPK_TRAIN = 200\n","cfg.MODEL.RPN.PRE_NMS_TOPK_TEST = 100\n","cfg.MODEL.RPN.POST_NMS_TOPK_TRAIN = 100\n","cfg.MODEL.RPN.POST_NMS_TOPK_TEST = 100\n","\n","cfg.MODEL.BACKBONE.FREEZE_AT = 2\n","\n","cfg.SOLVER.IMS_PER_BATCH = 4\n","cfg.SOLVER.BASE_LR = 0.005\n","cfg.SOLVER.STEPS = (11000, 14000)\n","cfg.SOLVER.MAX_ITER = 15000\n","cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64\n","cfg.MODEL.ROI_HEADS.NUM_CLASSES = NUM_CLASS\n","cfg.TEST.DETECTIONS_PER_IMAGE = 3\n","\n","model = build_model(cfg)\n","DetectionCheckpointer(model).load(cfg.MODEL.WEIGHTS)\n","model.eval()\n","\n","cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5"]},{"cell_type":"code","source":["import glob\n","import cv2\n","import numpy as np\n","\n","aug = ResizeShortestEdge([800, 800], 1333)\n","\n","def process_input(file_path):\n","  original_image = cv2.imread(file_path)\n","  height, width = original_image.shape[:2]\n","  image = aug.get_transform(original_image).apply_image(original_image)\n","  image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n","\n","  return {\"image\": image, \"height\": height, \"width\": width}\n","\n","inputs = []\n","for image_file in glob.glob('/content/MDP/ObjectDetection/images/train/*.JPG'):\n","  inputs.append(process_input(image_file))\n","\n","c2 = Caffe2Tracer(cfg, model, inputs[0:16])\n","script_model = c2.export_torchscript()\n","script_model.save('/content/model_cpu.pt')"],"metadata":{"id":"LmGFiy4i5xhV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9gI83ve0T4fx"},"source":["## Production"]},{"cell_type":"markdown","source":["### Preprocess"],"metadata":{"id":"rniervb42PPW"}},{"cell_type":"code","source":["import torch\n","import inspect, sys, pprint\n","import torch.nn.functional as F\n","from fvcore.transforms.transform import (\n","    BlendTransform,\n","    CropTransform,\n","    HFlipTransform,\n","    NoOpTransform,\n","    PadTransform,\n","    Transform,\n","    TransformList,\n","    VFlipTransform,\n",")\n","from PIL import Image\n","from typing import Any, List, Optional, Tuple, Union\n","\n","def _get_aug_input_args(aug, aug_input) -> List[Any]:\n","    if aug.input_args is None:\n","        # Decide what attributes are needed automatically\n","        prms = list(inspect.signature(aug.get_transform).parameters.items())\n","        # The default behavior is: if there is one parameter, then its \"image\"\n","        # (work automatically for majority of use cases, and also avoid BC breaking),\n","        # Otherwise, use the argument names.\n","        if len(prms) == 1:\n","            names = (\"image\",)\n","        else:\n","            names = []\n","            for name, prm in prms:\n","                if prm.kind in (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD):\n","                    raise TypeError(\n","                        f\"\"\" \\\n","The default implementation of `{type(aug)}.__call__` does not allow \\\n","`{type(aug)}.get_transform` to use variable-length arguments (*args, **kwargs)! \\\n","If arguments are unknown, reimplement `__call__` instead. \\\n","\"\"\"\n","                    )\n","                names.append(name)\n","        aug.input_args = tuple(names)\n","\n","    args = []\n","    for f in aug.input_args:\n","        try:\n","            args.append(getattr(aug_input, f))\n","        except AttributeError as e:\n","            raise AttributeError(\n","                f\"{type(aug)}.get_transform needs input attribute '{f}', \"\n","                f\"but it is not an attribute of {type(aug_input)}!\"\n","            ) from e\n","    return args\n","\n","class Augmentation:\n","    def _init(self, params=None):\n","        if params:\n","            for k, v in params.items():\n","                if k != \"self\" and not k.startswith(\"_\"):\n","                    setattr(self, k, v)\n","\n","    def get_transform(self, *args) -> Transform:\n","        raise NotImplementedError\n","\n","    def __call__(self, aug_input) -> Transform:\n","        args = _get_aug_input_args(self, aug_input)\n","        tfm = self.get_transform(*args)\n","        assert isinstance(tfm, (Transform, TransformList)), (\n","            f\"{type(self)}.get_transform must return an instance of Transform! \"\n","            f\"Got {type(tfm)} instead.\"\n","        )\n","        aug_input.transform(tfm)\n","        return tfm\n","\n","    def _rand_range(self, low=1.0, high=None, size=None):\n","        if high is None:\n","            low, high = 0, low\n","        if size is None:\n","            size = []\n","        return np.random.uniform(low, high, size)\n","\n","    def __repr__(self):\n","        try:\n","            sig = inspect.signature(self.__init__)\n","            classname = type(self).__name__\n","            argstr = []\n","            for name, param in sig.parameters.items():\n","                assert (\n","                    param.kind != param.VAR_POSITIONAL and param.kind != param.VAR_KEYWORD\n","                ), \"The default __repr__ doesn't support *args or **kwargs\"\n","                assert hasattr(self, name), (\n","                    \"Attribute {} not found! \"\n","                    \"Default __repr__ only works if attributes match the constructor.\".format(name)\n","                )\n","                attr = getattr(self, name)\n","                default = param.default\n","                if default is attr:\n","                    continue\n","                attr_str = pprint.pformat(attr)\n","                if \"\\n\" in attr_str:\n","                    # don't show it if pformat decides to use >1 lines\n","                    attr_str = \"...\"\n","                argstr.append(\"{}={}\".format(name, attr_str))\n","            return \"{}({})\".format(classname, \", \".join(argstr))\n","        except AssertionError:\n","            return super().__repr__()\n","\n","    __str__ = __repr__\n","\n","class ResizeTransform(Transform):\n","    def __init__(self, h, w, new_h, new_w, interp=None):\n","        super().__init__()\n","        if interp is None:\n","            interp = Image.BILINEAR\n","        self._set_attributes(locals())\n","\n","    def apply_image(self, img, interp=None):\n","        assert img.shape[:2] == (self.h, self.w)\n","        assert len(img.shape) <= 4\n","        interp_method = interp if interp is not None else self.interp\n","\n","        if img.dtype == np.uint8:\n","            if len(img.shape) > 2 and img.shape[2] == 1:\n","                pil_image = Image.fromarray(img[:, :, 0], mode=\"L\")\n","            else:\n","                pil_image = Image.fromarray(img)\n","            pil_image = pil_image.resize((self.new_w, self.new_h), interp_method)\n","            ret = np.asarray(pil_image)\n","            if len(img.shape) > 2 and img.shape[2] == 1:\n","                ret = np.expand_dims(ret, -1)\n","        else:\n","            # PIL only supports uint8\n","            if any(x < 0 for x in img.strides):\n","                img = np.ascontiguousarray(img)\n","            img = torch.from_numpy(img)\n","            shape = list(img.shape)\n","            shape_4d = shape[:2] + [1] * (4 - len(shape)) + shape[2:]\n","            img = img.view(shape_4d).permute(2, 3, 0, 1)  # hw(c) -> nchw\n","            _PIL_RESIZE_TO_INTERPOLATE_MODE = {\n","                Image.NEAREST: \"nearest\",\n","                Image.BILINEAR: \"bilinear\",\n","                Image.BICUBIC: \"bicubic\",\n","            }\n","            mode = _PIL_RESIZE_TO_INTERPOLATE_MODE[interp_method]\n","            align_corners = None if mode == \"nearest\" else False\n","            img = F.interpolate(\n","                img, (self.new_h, self.new_w), mode=mode, align_corners=align_corners\n","            )\n","            shape[:2] = (self.new_h, self.new_w)\n","            ret = img.permute(2, 3, 0, 1).view(shape).numpy()  # nchw -> hw(c)\n","\n","        return ret\n","\n","    def apply_coords(self, coords):\n","        coords[:, 0] = coords[:, 0] * (self.new_w * 1.0 / self.w)\n","        coords[:, 1] = coords[:, 1] * (self.new_h * 1.0 / self.h)\n","        return coords\n","\n","    def apply_segmentation(self, segmentation):\n","        segmentation = self.apply_image(segmentation, interp=Image.NEAREST)\n","        return segmentation\n","\n","    def inverse(self):\n","        return ResizeTransform(self.new_h, self.new_w, self.h, self.w, self.interp)\n","\n","class ResizeShortestEdge(Augmentation):\n","    @torch.jit.unused\n","    def __init__(\n","        self, short_edge_length, max_size=sys.maxsize, sample_style=\"range\", interp=Image.BILINEAR\n","    ):\n","        super().__init__()\n","        assert sample_style in [\"range\", \"choice\"], sample_style\n","\n","        self.is_range = sample_style == \"range\"\n","        if isinstance(short_edge_length, int):\n","            short_edge_length = (short_edge_length, short_edge_length)\n","        if self.is_range:\n","            assert len(short_edge_length) == 2, (\n","                \"short_edge_length must be two values using 'range' sample style.\"\n","                f\" Got {short_edge_length}!\"\n","            )\n","        self._init(locals())\n","\n","    @torch.jit.unused\n","    def get_transform(self, image):\n","        h, w = image.shape[:2]\n","        if self.is_range:\n","            size = np.random.randint(self.short_edge_length[0], self.short_edge_length[1] + 1)\n","        else:\n","            size = np.random.choice(self.short_edge_length)\n","        if size == 0:\n","            return NoOpTransform()\n","\n","        newh, neww = ResizeShortestEdge.get_output_shape(h, w, size, self.max_size)\n","        return ResizeTransform(h, w, newh, neww, self.interp)\n","\n","    @staticmethod\n","    def get_output_shape(\n","        oldh: int, oldw: int, short_edge_length: int, max_size: int\n","    ) -> Tuple[int, int]:\n","        h, w = oldh, oldw\n","        size = short_edge_length * 1.0\n","        scale = size / min(h, w)\n","        if h < w:\n","            newh, neww = size, scale * w\n","        else:\n","            newh, neww = scale * h, size\n","        if max(newh, neww) > max_size:\n","            scale = max_size * 1.0 / max(newh, neww)\n","            newh = newh * scale\n","            neww = neww * scale\n","        neww = int(neww + 0.5)\n","        newh = int(newh + 0.5)\n","        return (newh, neww)"],"metadata":{"id":"kgWWlzwr2TFD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from __future__ import division\n","from typing import Any, List, Tuple\n","from torch import device\n","from torch.nn import functional as F\n","from typing import List, Optional\n","\n","def shapes_to_tensor(x: List[int], device: Optional[torch.device] = None) -> torch.Tensor:\n","    \"\"\"\n","    Turn a list of integer scalars or integer Tensor scalars into a vector,\n","    in a way that's both traceable and scriptable.\n","    In tracing, `x` should be a list of scalar Tensor, so the output can trace to the inputs.\n","    In scripting or eager, `x` should be a list of int.\n","    \"\"\"\n","    if torch.jit.is_scripting():\n","        return torch.as_tensor(x, device=device)\n","    if torch.jit.is_tracing():\n","        assert all(\n","            [isinstance(t, torch.Tensor) for t in x]\n","        ), \"Shape should be tensor during tracing!\"\n","        # as_tensor should not be used in tracing because it records a constant\n","        ret = torch.stack(x)\n","        if ret.device != device:  # avoid recording a hard-coded device if not necessary\n","            ret = ret.to(device=device)\n","        return ret\n","    return torch.as_tensor(x, device=device)\n","\n","class ImageList(object):\n","    \"\"\"\n","    Structure that holds a list of images (of possibly\n","    varying sizes) as a single tensor.\n","    This works by padding the images to the same size.\n","    The original sizes of each image is stored in `image_sizes`.\n","    Attributes:\n","        image_sizes (list[tuple[int, int]]): each tuple is (h, w).\n","            During tracing, it becomes list[Tensor] instead.\n","    \"\"\"\n","\n","    def __init__(self, tensor: torch.Tensor, image_sizes: List[Tuple[int, int]]):\n","        \"\"\"\n","        Arguments:\n","            tensor (Tensor): of shape (N, H, W) or (N, C_1, ..., C_K, H, W) where K >= 1\n","            image_sizes (list[tuple[int, int]]): Each tuple is (h, w). It can\n","                be smaller than (H, W) due to padding.\n","        \"\"\"\n","        self.tensor = tensor\n","        self.image_sizes = image_sizes\n","\n","    def __len__(self) -> int:\n","        return len(self.image_sizes)\n","\n","    def __getitem__(self, idx) -> torch.Tensor:\n","        \"\"\"\n","        Access the individual image in its original size.\n","        Args:\n","            idx: int or slice\n","        Returns:\n","            Tensor: an image of shape (H, W) or (C_1, ..., C_K, H, W) where K >= 1\n","        \"\"\"\n","        size = self.image_sizes[idx]\n","        return self.tensor[idx, ..., : size[0], : size[1]]\n","\n","    @torch.jit.unused\n","    def to(self, *args: Any, **kwargs: Any) -> \"ImageList\":\n","        cast_tensor = self.tensor.to(*args, **kwargs)\n","        return ImageList(cast_tensor, self.image_sizes)\n","\n","    @property\n","    def device(self) -> device:\n","        return self.tensor.device\n","\n","    @staticmethod\n","    def from_tensors(\n","        tensors: List[torch.Tensor], size_divisibility: int = 0, pad_value: float = 0.0\n","    ) -> \"ImageList\":\n","        \"\"\"\n","        Args:\n","            tensors: a tuple or list of `torch.Tensor`, each of shape (Hi, Wi) or\n","                (C_1, ..., C_K, Hi, Wi) where K >= 1. The Tensors will be padded\n","                to the same shape with `pad_value`.\n","            size_divisibility (int): If `size_divisibility > 0`, add padding to ensure\n","                the common height and width is divisible by `size_divisibility`.\n","                This depends on the model and many models need a divisibility of 32.\n","            pad_value (float): value to pad\n","        Returns:\n","            an `ImageList`.\n","        \"\"\"\n","        assert len(tensors) > 0\n","        assert isinstance(tensors, (tuple, list))\n","        for t in tensors:\n","            assert isinstance(t, torch.Tensor), type(t)\n","            assert t.shape[:-2] == tensors[0].shape[:-2], t.shape\n","\n","        image_sizes = [(im.shape[-2], im.shape[-1]) for im in tensors]\n","        image_sizes_tensor = [shapes_to_tensor(x) for x in image_sizes]\n","        max_size = torch.stack(image_sizes_tensor).max(0).values\n","\n","        if size_divisibility > 1:\n","            stride = size_divisibility\n","            # the last two dims are H,W, both subject to divisibility requirement\n","            max_size = (max_size + (stride - 1)).div(stride, rounding_mode=\"floor\") * stride\n","\n","        # handle weirdness of scripting and tracing ...\n","        if torch.jit.is_scripting():\n","            max_size: List[int] = max_size.to(dtype=torch.long).tolist()\n","        else:\n","            if torch.jit.is_tracing():\n","                image_sizes = image_sizes_tensor\n","\n","        if len(tensors) == 1:\n","            # This seems slightly (2%) faster.\n","            # TODO: check whether it's faster for multiple images as well\n","            image_size = image_sizes[0]\n","            padding_size = [0, max_size[-1] - image_size[1], 0, max_size[-2] - image_size[0]]\n","            batched_imgs = F.pad(tensors[0], padding_size, value=pad_value).unsqueeze_(0)\n","        else:\n","            # max_size can be a tensor in tracing mode, therefore convert to list\n","            batch_shape = [len(tensors)] + list(tensors[0].shape[:-2]) + list(max_size)\n","            batched_imgs = tensors[0].new_full(batch_shape, pad_value)\n","            for img, pad_img in zip(tensors, batched_imgs):\n","                pad_img[..., : img.shape[-2], : img.shape[-1]].copy_(img)\n","\n","        return ImageList(batched_imgs.contiguous(), image_sizes)"],"metadata":{"id":"VnsQp1ub2aNe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Inference"],"metadata":{"id":"7vjJNKsV264k"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7S6FHvnbUKc_"},"outputs":[],"source":["import glob\n","import cv2\n","import numpy as np\n","\n","aug = ResizeShortestEdge([800, 800], 1333)\n","\n","def process_input(file_path):\n","  original_image = cv2.imread(file_path)\n","  height, width = original_image.shape[:2]\n","  image = aug.get_transform(original_image).apply_image(original_image)\n","  image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n","\n","  return {\"image\": image, \"height\": height, \"width\": width, 'name': file_path}\n","\n","inputs = []\n","for image_file in glob.glob('/content/MDP/ObjectDetection/images/train/*.JPG'):\n","  inputs.append(process_input(image_file))"]},{"cell_type":"code","source":["import torch\n","from torch import device\n","\n","torch.device('cpu')\n","\n","script_model = torch.load('/content/model_cpu.pt', map_location = torch.device('cpu'))\n","script_model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RpXaa5tg1bmw","executionInfo":{"status":"ok","timestamp":1645896099914,"user_tz":-480,"elapsed":898,"user":{"displayName":"Chua Cheng Hong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgB02yYr0OXz-1q3qlmnMPOfkLWcmGM_ckc4HHB=s64","userId":"15038074957184626047"}},"outputId":"e1e4692e-7f64-4a3f-d515-66867ff6931d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RecursiveScriptModule(\n","  original_name=Caffe2GeneralizedRCNN\n","  (_wrapped_model): RecursiveScriptModule(\n","    original_name=GeneralizedRCNN\n","    (backbone): RecursiveScriptModule(\n","      original_name=FPN\n","      (fpn_lateral2): RecursiveScriptModule(original_name=Conv2d)\n","      (fpn_output2): RecursiveScriptModule(original_name=Conv2d)\n","      (fpn_lateral3): RecursiveScriptModule(original_name=Conv2d)\n","      (fpn_output3): RecursiveScriptModule(original_name=Conv2d)\n","      (fpn_lateral4): RecursiveScriptModule(original_name=Conv2d)\n","      (fpn_output4): RecursiveScriptModule(original_name=Conv2d)\n","      (fpn_lateral5): RecursiveScriptModule(original_name=Conv2d)\n","      (fpn_output5): RecursiveScriptModule(original_name=Conv2d)\n","      (top_block): RecursiveScriptModule(original_name=LastLevelMaxPool)\n","      (bottom_up): RecursiveScriptModule(\n","        original_name=ResNet\n","        (stem): RecursiveScriptModule(\n","          original_name=BasicStem\n","          (conv1): RecursiveScriptModule(\n","            original_name=Conv2d\n","            (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","          )\n","        )\n","        (res2): RecursiveScriptModule(\n","          original_name=Sequential\n","          (0): RecursiveScriptModule(\n","            original_name=BottleneckBlock\n","            (shortcut): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv1): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv2): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv3): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","          )\n","          (1): RecursiveScriptModule(\n","            original_name=BottleneckBlock\n","            (conv1): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv2): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv3): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","          )\n","          (2): RecursiveScriptModule(\n","            original_name=BottleneckBlock\n","            (conv1): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv2): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv3): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","          )\n","        )\n","        (res3): RecursiveScriptModule(\n","          original_name=Sequential\n","          (0): RecursiveScriptModule(\n","            original_name=BottleneckBlock\n","            (shortcut): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv1): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv2): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv3): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","          )\n","          (1): RecursiveScriptModule(\n","            original_name=BottleneckBlock\n","            (conv1): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv2): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv3): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","          )\n","          (2): RecursiveScriptModule(\n","            original_name=BottleneckBlock\n","            (conv1): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv2): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv3): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","          )\n","          (3): RecursiveScriptModule(\n","            original_name=BottleneckBlock\n","            (conv1): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv2): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv3): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","          )\n","        )\n","        (res4): RecursiveScriptModule(\n","          original_name=Sequential\n","          (0): RecursiveScriptModule(\n","            original_name=BottleneckBlock\n","            (shortcut): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv1): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv2): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv3): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","          )\n","          (1): RecursiveScriptModule(\n","            original_name=BottleneckBlock\n","            (conv1): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv2): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv3): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","          )\n","          (2): RecursiveScriptModule(\n","            original_name=BottleneckBlock\n","            (conv1): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv2): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv3): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","          )\n","          (3): RecursiveScriptModule(\n","            original_name=BottleneckBlock\n","            (conv1): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv2): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv3): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","          )\n","          (4): RecursiveScriptModule(\n","            original_name=BottleneckBlock\n","            (conv1): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv2): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv3): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","          )\n","          (5): RecursiveScriptModule(\n","            original_name=BottleneckBlock\n","            (conv1): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv2): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv3): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","          )\n","        )\n","        (res5): RecursiveScriptModule(\n","          original_name=Sequential\n","          (0): RecursiveScriptModule(\n","            original_name=BottleneckBlock\n","            (shortcut): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv1): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv2): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv3): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","          )\n","          (1): RecursiveScriptModule(\n","            original_name=BottleneckBlock\n","            (conv1): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv2): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv3): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","          )\n","          (2): RecursiveScriptModule(\n","            original_name=BottleneckBlock\n","            (conv1): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv2): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","            (conv3): RecursiveScriptModule(\n","              original_name=Conv2d\n","              (norm): RecursiveScriptModule(original_name=FrozenBatchNorm2d)\n","            )\n","          )\n","        )\n","      )\n","    )\n","    (proposal_generator): RecursiveScriptModule(\n","      original_name=Caffe2RPN\n","      (rpn_head): RecursiveScriptModule(\n","        original_name=StandardRPNHead\n","        (conv): RecursiveScriptModule(\n","          original_name=Conv2d\n","          (activation): RecursiveScriptModule(original_name=ReLU)\n","        )\n","        (objectness_logits): RecursiveScriptModule(original_name=Conv2d)\n","        (anchor_deltas): RecursiveScriptModule(original_name=Conv2d)\n","      )\n","      (anchor_generator): RecursiveScriptModule(\n","        original_name=DefaultAnchorGenerator\n","        (cell_anchors): RecursiveScriptModule(original_name=BufferList)\n","      )\n","    )\n","    (roi_heads): RecursiveScriptModule(\n","      original_name=StandardROIHeads\n","      (box_pooler): RecursiveScriptModule(\n","        original_name=Caffe2ROIPooler\n","        (level_poolers): RecursiveScriptModule(\n","          original_name=ModuleList\n","          (0): RecursiveScriptModule(original_name=ROIAlign)\n","          (1): RecursiveScriptModule(original_name=ROIAlign)\n","          (2): RecursiveScriptModule(original_name=ROIAlign)\n","          (3): RecursiveScriptModule(original_name=ROIAlign)\n","        )\n","      )\n","      (box_head): RecursiveScriptModule(\n","        original_name=FastRCNNConvFCHead\n","        (flatten): RecursiveScriptModule(original_name=Flatten)\n","        (fc1): RecursiveScriptModule(original_name=Linear)\n","        (fc_relu1): RecursiveScriptModule(original_name=ReLU)\n","        (fc2): RecursiveScriptModule(original_name=Linear)\n","        (fc_relu2): RecursiveScriptModule(original_name=ReLU)\n","      )\n","      (box_predictor): RecursiveScriptModule(\n","        original_name=FastRCNNOutputLayers\n","        (cls_score): RecursiveScriptModule(original_name=Linear)\n","        (bbox_pred): RecursiveScriptModule(original_name=Linear)\n","      )\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["from typing import Tuple\n","from google.colab.patches import cv2_imshow\n","\n","\"\"\"\n","Convert pytorch-style structured inputs to caffe2-style inputs that\n","are tuples of tensors.\n","Args:\n","    batched_inputs (list[dict]): inputs to a detectron2 model\n","        in its standard format. Each dict has \"image\" (CHW tensor), and optionally\n","        \"height\" and \"width\".\n","Returns:\n","    tuple[Tensor]:\n","        tuple of tensors that will be the inputs to the\n","        :meth:`forward` method. For existing models, the first\n","        is an NCHW tensor (padded and batched); the second is\n","        a im_info Nx3 tensor, where the rows are\n","        (height, width, unused legacy parameter)\n","\"\"\"\n","\n","device = 'cpu'\n","size_divisibility = 32\n","for input in inputs:\n","  print(input[\"name\"])\n","  batched_inputs = [input]\n","  images = [x[\"image\"] for x in batched_inputs]\n","  images = ImageList.from_tensors(images, size_divisibility)\n","\n","  im_info = []\n","  for input_per_image, image_size in zip(batched_inputs, images.image_sizes):\n","    target_height = input_per_image.get(\"height\", image_size[0])\n","    target_width = input_per_image.get(\"width\", image_size[1])  # noqa\n","    scale = target_height / image_size[0]\n","    im_info.append([image_size[0], image_size[1], scale])\n","  im_info = torch.Tensor(im_info)\n","\n","  print(script_model(tuple([images.tensor.to(device), im_info.to(device)])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jzORQgr9yrOQ","executionInfo":{"status":"ok","timestamp":1645896918596,"user_tz":-480,"elapsed":119568,"user":{"displayName":"Chua Cheng Hong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgB02yYr0OXz-1q3qlmnMPOfkLWcmGM_ckc4HHB=s64","userId":"15038074957184626047"}},"outputId":"628f5bea-43aa-471a-9d1e-207163f03c8a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/MDP/ObjectDetection/images/train/Stop_40.JPG\n","(tensor([[400.0467, 220.1270, 734.8254, 577.2534]]), tensor([0.9981]), tensor([29.]))\n","/content/MDP/ObjectDetection/images/train/AlphabetB_21.JPG\n","(tensor([[404.5714, 235.8521, 737.9523, 559.8884],\n","        [423.9288, 228.0222, 733.8324, 559.0939]]), tensor([0.9548, 0.1434]), tensor([11., 17.]))\n","/content/MDP/ObjectDetection/images/train/AlphabetV_31.JPG\n","(tensor([[362.0975, 241.2996, 703.1246, 591.2546]]), tensor([0.9955]), tensor([24.]))\n","/content/MDP/ObjectDetection/images/train/two_12.JPG\n","(tensor([[347.9690, 257.6335, 720.7661, 618.7044],\n","        [345.4269, 230.3823, 726.9185, 631.8829]]), tensor([0.9553, 0.3317]), tensor([ 2., 10.]))\n","/content/MDP/ObjectDetection/images/train/nine_19.JPG\n","(tensor([[346.3542, 285.0568, 718.4507, 636.5948],\n","        [359.7069, 294.6569, 692.1418, 643.4203]]), tensor([0.8147, 0.1569]), tensor([ 6., 12.]))\n","/content/MDP/ObjectDetection/images/train/five_15.JPG\n","(tensor([[371.7115, 244.7500, 737.5570, 611.5439],\n","        [370.5366, 218.0696, 734.3458, 608.2538],\n","        [370.4648, 204.5751, 733.8428, 608.4049]]), tensor([0.9773, 0.0563, 0.2113]), tensor([ 5., 12., 22.]))\n","/content/MDP/ObjectDetection/images/train/seven_17.JPG\n","(tensor([[388.8280, 252.4382, 713.6614, 572.8932]]), tensor([0.9984]), tensor([7.]))\n","/content/MDP/ObjectDetection/images/train/AlphabetA_20.JPG\n","(tensor([[357.1385, 236.4678, 731.2993, 614.1701]]), tensor([0.9888]), tensor([10.]))\n","/content/MDP/ObjectDetection/images/train/three_13.JPG\n","(tensor([[384.6559, 255.8592, 737.0359, 620.0164],\n","        [403.1252, 275.2443, 730.4113, 616.3958],\n","        [383.6878, 262.8482, 739.4370, 622.3702]]), tensor([0.3894, 0.3935, 0.0910]), tensor([ 3.,  8., 29.]))\n","/content/MDP/ObjectDetection/images/train/six_16.JPG\n","(tensor([[372.5952, 236.7259, 735.4343, 591.7562],\n","        [374.8506, 252.9331, 731.6868, 578.8435],\n","        [388.1445, 240.8188, 726.0235, 567.9690]]), tensor([0.4940, 0.2667, 0.4661]), tensor([ 6.,  9., 21.]))\n","/content/MDP/ObjectDetection/images/train/left_39.JPG\n","(tensor([[430.2451, 220.5296, 787.6199, 571.6017],\n","        [433.8984, 193.7171, 799.5414, 582.7170]]), tensor([0.9918, 0.0681]), tensor([19., 25.]))\n","/content/MDP/ObjectDetection/images/train/AlphabetD_23.JPG\n","(tensor([[389.5259, 225.4574, 714.7004, 558.1975],\n","        [387.2295, 167.0896, 744.0212, 582.3621],\n","        [384.8990, 166.8106, 741.3428, 580.2449]]), tensor([0.9399, 0.0567, 0.3272]), tensor([13., 18., 22.]))\n","/content/MDP/ObjectDetection/images/train/up_36.JPG\n","(tensor([[350.8315, 181.6209, 770.7303, 638.2863],\n","        [372.2322, 218.0147, 740.2836, 597.0585],\n","        [372.9357, 179.4126, 753.4717, 629.6013]]), tensor([0.2215, 0.9208, 0.0632]), tensor([10., 23., 24.]))\n","/content/MDP/ObjectDetection/images/train/AlphabetH_27.JPG\n","(tensor([[387.9219, 241.0934, 769.6371, 594.9779]]), tensor([0.9914]), tensor([18.]))\n","/content/MDP/ObjectDetection/images/train/AlphabetY_34.JPG\n","(tensor([[359.7508, 229.5116, 732.5457, 587.2886]]), tensor([0.9871]), tensor([27.]))\n","/content/MDP/ObjectDetection/images/train/AlphabetC_22.JPG\n","(tensor([[370.6786, 181.4253, 748.0366, 592.6987],\n","        [385.6840, 233.6689, 727.4758, 580.6833],\n","        [376.4378, 165.7026, 734.5457, 586.9748]]), tensor([0.1055, 0.9612, 0.1126]), tensor([ 6., 12., 22.]))\n","/content/MDP/ObjectDetection/images/train/AlphabetG_26.JPG\n","(tensor([[374.9641, 254.4388, 696.4268, 577.9363],\n","        [349.1713, 147.4046, 713.8873, 593.1934]]), tensor([0.9961, 0.1795]), tensor([17., 22.]))\n","/content/MDP/ObjectDetection/images/train/right_38.JPG\n","(tensor([[361.3393, 207.5576, 733.7214, 588.9230],\n","        [371.1175, 219.8736, 727.1228, 592.4218],\n","        [368.5656, 170.7309, 742.7479, 595.0632]]), tensor([0.3060, 0.9667, 0.2660]), tensor([14., 20., 24.]))\n","/content/MDP/ObjectDetection/images/train/eight_18.JPG\n","(tensor([[375.7418, 264.0566, 691.9160, 592.7512]]), tensor([0.9566]), tensor([8.]))\n","/content/MDP/ObjectDetection/images/train/AlphabetX_33.JPG\n","(tensor([[339.0441, 174.7148, 713.9697, 625.1400],\n","        [343.1843, 254.6846, 709.2083, 603.6943]]), tensor([0.2131, 0.9892]), tensor([24., 26.]))\n","/content/MDP/ObjectDetection/images/train/AlphabetE_24.JPG\n","(tensor([[362.6015, 194.9806, 775.8385, 583.7433],\n","        [409.2395, 201.0058, 744.5961, 575.8092],\n","        [377.9566, 183.3997, 749.3732, 589.5172]]), tensor([0.1142, 0.9714, 0.1993]), tensor([10., 15., 18.]))\n","/content/MDP/ObjectDetection/images/train/bullseye.JPG\n","(tensor([[351.0943, 219.5347, 736.0697, 623.0662],\n","        [347.3565, 226.0806, 737.9662, 624.6085]]), tensor([0.0531, 0.9781]), tensor([18., 30.]))\n","/content/MDP/ObjectDetection/images/train/AlphabetF_25.JPG\n","(tensor([[371.3214, 270.7778, 693.1589, 631.1797],\n","        [357.4662, 282.8950, 709.2402, 606.1851],\n","        [362.9481, 273.2795, 715.4922, 608.2184]]), tensor([0.3071, 0.2571, 0.1893]), tensor([14., 16., 18.]))\n","/content/MDP/ObjectDetection/images/train/AlphabetT_29.JPG\n","(tensor([[403.7488, 247.8408, 725.5700, 555.1856],\n","        [389.8518, 252.1285, 737.4137, 564.5429],\n","        [388.1194, 239.5829, 720.4116, 574.4990]]), tensor([0.1208, 0.3038, 0.4421]), tensor([16., 18., 27.]))\n","/content/MDP/ObjectDetection/images/train/four_14.JPG\n","(tensor([[386.0133, 283.1327, 749.5064, 620.1617],\n","        [378.8223, 261.9878, 760.5305, 622.7144],\n","        [386.2333, 235.0909, 740.7585, 631.2476]]), tensor([0.9360, 0.4841, 0.0555]), tensor([ 4., 10., 25.]))\n","/content/MDP/ObjectDetection/images/train/down_37.JPG\n","(tensor([[409.4541, 208.9996, 735.2267, 560.5927]]), tensor([0.9853]), tensor([14.]))\n","/content/MDP/ObjectDetection/images/train/AlphabetU_30.JPG\n","(tensor([[373.8667, 251.3814, 737.7955, 599.5455]]), tensor([0.9957]), tensor([22.]))\n","/content/MDP/ObjectDetection/images/train/one_11.JPG\n","(tensor([[398.3381, 246.6797, 757.1309, 626.9705]]), tensor([0.9912]), tensor([1.]))\n","/content/MDP/ObjectDetection/images/train/AlphabetW_32.JPG\n","(tensor([[359.1828, 243.9521, 708.7649, 586.5239]]), tensor([0.9952]), tensor([25.]))\n","/content/MDP/ObjectDetection/images/train/AlphabetZ_35.JPG\n","(tensor([[357.1880, 261.3628, 708.1522, 637.5659],\n","        [360.9715, 282.1336, 706.5071, 622.8141],\n","        [363.6653, 288.7217, 705.0673, 624.0693]]), tensor([0.1673, 0.1328, 0.6187]), tensor([10., 14., 28.]))\n","/content/MDP/ObjectDetection/images/train/AlphabetS_28.JPG\n","(tensor([[357.3290, 236.3163, 745.4518, 636.0073],\n","        [344.5162, 231.2481, 754.8021, 639.6891]]), tensor([0.9666, 0.0857]), tensor([21., 26.]))\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kk8L6dfLOjlh"},"outputs":[],"source":["!cp /content/model.ts \"/content/drive/MyDrive/Colab Notebooks/custom_data/mdp_models\""]},{"cell_type":"markdown","metadata":{"id":"4SlbATC-O6Uo"},"source":["## Checking of predictions as required"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i9nqbMrlLKAp"},"outputs":[],"source":["from detectron2.utils.visualizer import Visualizer\n","from google.colab.patches import cv2_imshow\n","\n","for img_file, infer_result in history.items():\n","  img = cv2.imread(img_file + '.jpg')\n","\n","  v = Visualizer(img[:, :, ::-1])\n","  out = v.draw_instance_predictions(infer_result)\n","  cv2_imshow(out.get_image()[:, :, ::-1])"]},{"cell_type":"markdown","metadata":{"id":"x0zB8090ujHG"},"source":["## Commit"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2635,"status":"ok","timestamp":1645719070588,"user":{"displayName":"Chua Cheng Hong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgB02yYr0OXz-1q3qlmnMPOfkLWcmGM_ckc4HHB=s64","userId":"15038074957184626047"},"user_tz":-480},"id":"71zHBU2UujVH","outputId":"01ea3212-031a-471e-94f1-549aa7f93ef6"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/MDP\n","[main ce3810c] added new api methods for server\n"," 1 file changed, 1 insertion(+), 1 deletion(-)\n"," rewrite ObjectDetection/api_server.ipynb (82%)\n","Already up to date.\n","Counting objects: 4, done.\n","Delta compression using up to 2 threads.\n","Compressing objects: 100% (4/4), done.\n","Writing objects: 100% (4/4), 915 bytes | 915.00 KiB/s, done.\n","Total 4 (delta 3), reused 0 (delta 0)\n","remote: Resolving deltas: 100% (3/3), completed with 3 local objects.\u001b[K\n","remote: This repository moved. Please use the new location:\u001b[K\n","remote:   https://github.com/thecch/MDP.git\u001b[K\n","To https://github.com/burn874/MDP.git\n","   97d2189..ce3810c  main -> main\n","/content\n"]}],"source":["!rm -f /content/MDP/ObjectDetection/\"api_server.ipynb\"\n","!rm -rf /content/MDP/ObjectDetection/models/output/*\n","!cp /content/drive/MyDrive/\"Colab Notebooks\"/api_server.ipynb /content/MDP/ObjectDetection/\"api_server.ipynb\"\n","%cd /content/MDP\n","!git add --all\n","!git commit -m \"added export for detectron2\"\n","!git pull\n","!git push\n","%cd /content"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F90lbS68jfep"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UzdbU8w3jfhA"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V8G6_UE5jfjM"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZlCn2WDCjfli"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DG1wbc9RciTv"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"omVlv3lMciWB"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b82SXLimciYh"},"outputs":[],"source":[""]}],"metadata":{"colab":{"collapsed_sections":[],"name":"api_server.ipynb","provenance":[{"file_id":"1tU6RnSi7XBJFzJCpAjDPb3GSq-bIiwrV","timestamp":1643651546812},{"file_id":"16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5","timestamp":1643632837444}],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}