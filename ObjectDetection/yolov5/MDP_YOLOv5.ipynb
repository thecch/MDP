{"cells":[{"cell_type":"markdown","metadata":{"id":"yNveqeA1KXGy"},"source":["# Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21450,"status":"ok","timestamp":1646830265767,"user":{"displayName":"Chua Cheng Hong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgB02yYr0OXz-1q3qlmnMPOfkLWcmGM_ckc4HHB=s64","userId":"15038074957184626047"},"user_tz":-480},"id":"zv9r7Ha7mNmM","outputId":"8d682590-6b61-4aaa-9a2b-c9fd65ab8953"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":158086,"status":"ok","timestamp":1646830423847,"user":{"displayName":"Chua Cheng Hong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgB02yYr0OXz-1q3qlmnMPOfkLWcmGM_ckc4HHB=s64","userId":"15038074957184626047"},"user_tz":-480},"id":"kTvDNSILZoN9"},"outputs":[],"source":["%%capture\n","!wget https://packagecloud.io/github/git-lfs/packages/debian/bullseye/git-lfs_3.1.2_amd64.deb/download -O /content/lfs.deb\n","!apt-get install /content/lfs.deb\n","!rm -f /content/lfs.deb\n","\n","!git config --global include.path \"/content/drive/MyDrive/Colab Notebooks/.gitconfig\"\n","# connect to github\n","%cd /content/\n","!gituser=\"$(git config --get user.name)\";\\\n","  gitpassword=\"$(git config --get user.password)\";\\\n","  git clone \"https://${gituser}:${gitpassword}@github.com/thecch/MDP.git\";\\\n","# !cd MDP;git lfs pull --include \"*.onnx\"\n","\n","# clone YOLOv5 \n","!git clone https://github.com/ultralytics/yolov5  # clone repo\n","%cd yolov5\n","%pip install -qr requirements.txt # install dependencies\n","%pip install -q roboflow\n","\n","# download data\n","import os\n","from roboflow import Roboflow\n","rf = Roboflow(api_key=\"yDOW7Qj0fDI6hlfSeeuv\")\n","project = rf.workspace(\"mdp-7hpg1\").project(\"mdpv2-jutfa\")\n","dataset = project.version(1).download(\"yolov5\")"]},{"cell_type":"markdown","metadata":{"id":"X7yAi9hd-T4B"},"source":["# Train model\n","\n","Here, we are able to pass a number of arguments:\n","- **img:** define input image size\n","- **batch:** determine batch size\n","- **epochs:** define the number of training epochs. (Note: often, 3000+ are common here!)\n","- **data:** Our dataset locaiton is saved in the `dataset.location`\n","- **weights:** specify a path to weights to start transfer learning from. Here we choose the generic COCO pretrained checkpoint.\n","- **cache:** cache images for faster training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"dAPrXnE_56BZ"},"outputs":[],"source":["resume_checkpoint = False\n","if resume_checkpoint:\n","  os.system('cp -rT /content/drive/MyDrive/Colab\\ Notebooks/custom_data/mdp_models/checkpoint/exp /content/MDP/ObjectDetection/yolov5/checkpoint/exp')\n","else:\n","  os.system('rm -rf /content/MDP/ObjectDetection/yolov5/checkpoint/*')\n","\n","# Transfer learning by initializing pretrained model weights\n","!python /content/yolov5/train.py\\\n","  --data {dataset.location}/data.yaml\\\n","  --project /content/MDP/ObjectDetection/yolov5/checkpoint/\\\n","  --hyp /content/yolov5/data/hyps/hyp.scratch-low.yaml\\\n","  --exist-ok\\\n","  --cache\\\n","  { \"--resume /content/MDP/ObjectDetection/yolov5/checkpoint/exp/weights/last.pt\" if resume_checkpoint else \"--weights  yolov5m.pt\" }\\\n","  --batch-size -1\\\n","  --img 640\\\n","  --epochs 100"]},{"cell_type":"markdown","metadata":{"id":"jtmS7_TXFsT3"},"source":["# Export model"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":19810,"status":"ok","timestamp":1646832873124,"user":{"displayName":"Chua Cheng Hong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgB02yYr0OXz-1q3qlmnMPOfkLWcmGM_ckc4HHB=s64","userId":"15038074957184626047"},"user_tz":-480},"id":"7iiObB2WCMh6"},"outputs":[],"source":["import subprocess\n","\n","model_path_list = [\n","  # ('/content/MDP/ObjectDetection/models/backup/yolov5m_main.pt', '/content/MDP/ObjectDetection/models/yolov5m_main.onnx'),\n","  # ('/content/MDP/ObjectDetection/models/backup/yolov5m_bak1.pt', '/content/MDP/ObjectDetection/models/yolov5m_bak1.onnx')\n","  # ('/content/MDP/ObjectDetection/models/backup/yolov5m_bak2.pt', '/content/MDP/ObjectDetection/models/yolov5m_bak2.onnx'),\n","  ('/content/MDP/ObjectDetection/models/backup/yolov5s_bak3.pt', '/content/MDP/ObjectDetection/models/yolov5s_bak3.onnx')\n","]\n","\n","def export_model(input_model_path, output_model_path):\n","  subprocess.run([\n","    'python', '/content/yolov5/export.py',\n","    '--weights', input_model_path, \n","    '--data', dataset.location + '/data.yml', \n","    '--device', 'cpu',\n","    '--include', 'onnx',\n","    '--batch-size', '1',\n","    '--simplify'\n","  ])\n","  subprocess.run(['mv', input_model_path[0:-2] + 'onnx', output_model_path])\n","  subprocess.run(['mv', input_model_path, '/content/MDP/ObjectDetection/models/backup/{}.pt'.format(output_model_path.split('/')[-1].split('.')[0])])\n","\n","for idx, (input_model_path, output_model_path) in enumerate(model_path_list):\n","  export_model(input_model_path, output_model_path)"]},{"cell_type":"code","source":[""],"metadata":{"id":"VPiOpM1O43fX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"08QJROA6rqh4"},"source":["# Inference with exported model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LFk4BsTRGPtL"},"outputs":[],"source":["os.environ['MODELS'] = ' '.join([\n","  # '/content/MDP/ObjectDetection/models/backup/yolov5m_main.pt',\n","  # '/content/MDP/ObjectDetection/models/backup/yolov5m_bak1.pt',\n","  # '/content/MDP/ObjectDetection/models/backup/yolov5m_bak2.pt',\n","  '/content/yolov5x_bak3.pt'\n","])\n","\n","!for model in $MODELS; do\\\n","  python /content/yolov5/detect.py\\\n","    --weights $model\\\n","    --conf-thres 0.6\\\n","    --img 640 640\\\n","    --device '0'\\\n","    --source /content/yolov5/MDPv2-1/test/images/;\\\n","  done"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"bHKjppicud0R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646832353815,"user_tz":-480,"elapsed":24742,"user":{"displayName":"Chua Cheng Hong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgB02yYr0OXz-1q3qlmnMPOfkLWcmGM_ckc4HHB=s64","userId":"15038074957184626047"}},"outputId":"1e51b49c-b7c0-4702-9300-f687d1f49ab6"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mval: \u001b[0mdata=/content/yolov5/MDPv2-1/data.yaml, weights=['/content/yolov5x_bak3.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=True, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n","YOLOv5 ðŸš€ v6.1-21-ge6e36aa torch 1.10.0+cu111 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n","\n","Fusing layers... \n","Model Summary: 224 layers, 7134820 parameters, 0 gradients\n","\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/yolov5/MDPv2-1/valid/labels.cache' images and labels... 697 found, 0 missing, 0 empty, 0 corrupt: 100% 697/697 [00:00<?, ?it/s]\n","               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 22/22 [00:13<00:00,  1.63it/s]\n","                 all        697        697     0.0235     0.0281     0.0308    0.00412\n","            up_arrow        697         30          0          0          0          0\n","          down_arrow        697         23          0          0          0          0\n","         right_arrow        697         18          0          0          0          0\n","          left_arrow        697         21          0          0     0.0154    0.00175\n","                stop        697         18          0          0          0          0\n","                 one        697         45          0          0       0.02    0.00409\n","                 two        697         19          0          0          0          0\n","               three        697         21          0          0    0.00464    0.00107\n","                four        697         24          0          0          0          0\n","                five        697         29          0          0     0.0587    0.00777\n","                 six        697         22          0          0     0.0105     0.0013\n","               seven        697         13          0          0     0.0211    0.00525\n","               eight        697         25          0          0     0.0308    0.00326\n","                nine        697         19          0          0    0.00922    0.00206\n","           alphabetA        697         26          0          0     0.0197    0.00424\n","           alphabetB        697         25          0          0   0.000772   7.72e-05\n","           alphabetC        697         23          0          0     0.0177     0.0039\n","           alphabetD        697         28          0          0    0.00521   0.000612\n","           alphabetE        697         15          0          0     0.0168      0.003\n","           alphabetF        697         19          0          0    0.00865   0.000953\n","           alphabetG        697         16          0          0     0.0727    0.00897\n","           alphabetH        697         23          0          0          0          0\n","           alphabetS        697         19      0.479      0.632      0.416     0.0416\n","           alphabetT        697         15          0          0          0          0\n","           alphabetU        697         21      0.249      0.238     0.0645    0.00693\n","           alphabetV        697         13          0          0    0.00505   0.000668\n","           alphabetW        697         22          0          0     0.0147    0.00147\n","           alphabetX        697         25          0          0     0.0506     0.0121\n","           alphabetY        697         28          0          0     0.0895     0.0162\n","           alphabetZ        697         34          0          0          0          0\n","             bulleye        697         18          0          0     0.0024   0.000493\n","Speed: 0.1ms pre-process, 3.5ms inference, 1.6ms NMS per image at shape (32, 3, 640, 640)\n","Results saved to \u001b[1mruns/val/exp2\u001b[0m\n","697 labels saved to runs/val/exp2/labels\n"]}],"source":["!python /content/yolov5/val.py\\\n","  --weights $MODELS\\\n","  --data /content/yolov5/MDPv2-1/data.yaml\\\n","  --img 640\\\n","  --save-txt"]},{"cell_type":"markdown","metadata":{"id":"bnQHFgB5sLW-"},"source":["# Commit"]},{"cell_type":"code","execution_count":77,"metadata":{"id":"l9d4zNG7rtRK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646835643606,"user_tz":-480,"elapsed":32740,"user":{"displayName":"Chua Cheng Hong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgB02yYr0OXz-1q3qlmnMPOfkLWcmGM_ckc4HHB=s64","userId":"15038074957184626047"}},"outputId":"4895e25d-16bb-4b8a-82e7-8cdf3b7197c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/MDP\n","\"*.onnx\" already supported\n","[main a5cdeea] move model out of lfs\n"," 4 files changed, 0 insertions(+), 0 deletions(-)\n"," create mode 100644 ObjectDetection/models/backup/yolov5m_bak1.pt\n"," create mode 100644 ObjectDetection/models/backup/yolov5m_bak2.pt\n"," create mode 100644 ObjectDetection/models/backup/yolov5m_main.pt\n"," create mode 100644 ObjectDetection/models/backup/yolov5s_bak3.pt\n","Already up to date.\n","Counting objects: 9, done.\n","Delta compression using up to 4 threads.\n","Compressing objects: 100% (9/9), done.\n","Writing objects: 100% (9/9), 124.53 MiB | 7.89 MiB/s, done.\n","Total 9 (delta 3), reused 2 (delta 0)\n","remote: Resolving deltas: 100% (3/3), completed with 3 local objects.\u001b[K\n","To https://github.com/thecch/MDP.git\n","   61ad1f1..a5cdeea  main -> main\n","/content\n"]}],"source":["!rm -rf \"/content/MDP/ObjectDetection/yolov5/MDP_YOLOv5.ipynb\" /content/MDP/ObjectDetection/yolov5/checkpoint/*\n","!cp \"/content/drive/MyDrive/Colab Notebooks/MDP_YOLOv5.ipynb\" \"/content/MDP/ObjectDetection/yolov5/MDP_YOLOv5.ipynb\"\n","!cp /content/MDP/ObjectDetection/models/*.onnx /content/drive/MyDrive/Colab\\ Notebooks/custom_data/mdp_models/onnx/.\n","!cp /content/MDP/ObjectDetection/models/backup/*.pt /content/drive/MyDrive/Colab\\ Notebooks/custom_data/mdp_models/backup/.\n","\n","%cd /content/MDP\n","!git lfs untrack \"*.onnx\"\n","!git lfs track \"yolov5m_*.onnx\"\n","!git add --all\n","!git commit -m \"move model out of lfs\"\n","!git pull\n","!git push\n","%cd /content"]},{"cell_type":"code","source":[""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uKGDnrMhDasO","executionInfo":{"status":"ok","timestamp":1646835674445,"user_tz":-480,"elapsed":459,"user":{"displayName":"Chua Cheng Hong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgB02yYr0OXz-1q3qlmnMPOfkLWcmGM_ckc4HHB=s64","userId":"15038074957184626047"}},"outputId":"9d9d5c55-d23c-4131-938c-f780dfaf4d0d"},"execution_count":79,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/MDP\n","Untracking \"*.onnx\"\n","Tracking \"yolov5m_*.onnx\"\n"]}]},{"cell_type":"code","execution_count":74,"metadata":{"id":"DWZXvITMHuE3","executionInfo":{"status":"ok","timestamp":1646835567276,"user_tz":-480,"elapsed":517,"user":{"displayName":"Chua Cheng Hong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgB02yYr0OXz-1q3qlmnMPOfkLWcmGM_ckc4HHB=s64","userId":"15038074957184626047"}}},"outputs":[],"source":["!rm -f /content/MDP/ObjectDetection/models/backup/*"]},{"cell_type":"code","execution_count":76,"metadata":{"id":"rs41CCBrHuHE","executionInfo":{"status":"ok","timestamp":1646835607148,"user_tz":-480,"elapsed":490,"user":{"displayName":"Chua Cheng Hong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgB02yYr0OXz-1q3qlmnMPOfkLWcmGM_ckc4HHB=s64","userId":"15038074957184626047"}}},"outputs":[],"source":["!cp /content/temp/* /content/MDP/ObjectDetection/models/backup/."]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1396,"status":"ok","timestamp":1646833837527,"user":{"displayName":"Chua Cheng Hong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgB02yYr0OXz-1q3qlmnMPOfkLWcmGM_ckc4HHB=s64","userId":"15038074957184626047"},"user_tz":-480},"id":"QCQkdigqUM4w","outputId":"862f161e-f076-4e2e-a497-1afaf6c48102"},"outputs":[{"output_type":"stream","name":"stdout","text":["[CVU-Info] Backend: Onnx-1.10.0-cpu\n","[CVU-Info] Backend: Onnx-1.10.0-cpu\n","[CVU-Info] Backend: Onnx-1.10.0-cpu\n","[CVU-Info] Backend: Onnx-1.10.0-cpu\n"]}],"source":["import os, sys\n","import glob\n","import time\n","import numpy as np\n","import pandas as pd\n","import cv2\n","import onnxruntime\n","from cvu.detector.yolov5 import Yolov5 as Yolov5Onnx\n","from pathlib import Path\n","from shapely.geometry import Polygon\n","\n","COLAB_MODE = True\n","THRESHOLD = 0.5\n","__file__ = '/content/MDP/ObjectDetection/onnx_predictor.py' if COLAB_MODE else __file__\n","BASE_PATH = os.path.split(os.path.realpath(__file__))[0]\n","\n","class_map = pd.read_csv(BASE_PATH + '/class_map.csv')\n","\n","models = []\n","MODEL_DICT = dict()\n","for model_path in glob.glob(BASE_PATH + '/models/*.onnx'): \n","  model_name = model_path.split('/')[-1][8:12]\n","  models.append(model_name)\n","  MODEL_DICT[model_name] = Yolov5Onnx(\n","    classes = class_map[model_name].sort_values().astype('str').to_list(), backend = \"onnx\",\n","    weight = '{}/models/{}'.format(BASE_PATH, model_path.split('/')[-1]), device = 'cpu'\n","  )\n","\n","def get_preds(image_path):\n","  \n","  preds_list = []\n","  for (model_name, model) in MODEL_DICT.items():\n","    image = cv2.imread(image_path)\n","    image = cv2.resize(image, (640, 640))\n","    cur_output_path = image_path.split(os.sep)\n","    cur_output_path[-2] = 'output'\n","    cur_output_path[-1] = model_name + '_' + cur_output_path[-1]\n","    cur_output_path = '/'.join(cur_output_path)\n","\n","    preds = model(image)\n","    preds.draw(image)\n","    cv2.imwrite(cur_output_path, image)\n","    preds_list.append([model_name, preds])\n","  \n","  orignal_image = cv2.imread(image_path)\n","  orignal_image = cv2.resize(orignal_image, (640, 640))\n","  output_path = image_path.split(os.sep)\n","  output_path[-2] = 'output'\n","  output_path[-1] = 'combined_' + output_path[-1]\n","  output_path = '/'.join(output_path)\n","\n","  for model_preds in preds_list:\n","    model_preds[1].draw(orignal_image)\n","  cv2.imwrite(output_path, orignal_image)\n","\n","  return preds_list\n","\n","def process_pred(model_name, pred):\n","  return {\n","    'model': model_name,\n","    'bbox': pred.bbox,\n","    'conf': pred.confidence,\n","    'id': class_map[class_map[model_name].astype('str') == str(pred.class_name)].iloc[0].id\n","  }\n","  \n","def process_rows(df_row):\n","  df_row['model1'], df_row['model2'] = sorted([str(df_row.model_x), str(df_row.model_y)])\n","  df_row['conf1'], df_row['conf2'] = sorted([float(df_row.conf_x), float(df_row.conf_y)])\n","  df_row['bbox1'], df_row['bbox2'] = df_row.bbox_x, df_row.bbox_y\n","  df_row['weight1'] = 1 if df_row.model_x == 'main' else 0.75\n","  df_row['weight2'] = 1 if df_row.model_y == 'main' else 0.75\n","  return df_row[['id', 'model1', 'model2', 'conf1', 'conf2', 'bbox1', 'bbox2', 'weight1', 'weight2']]\n","\n","def calc_score(df_row):\n","  x1, y1, x2, y2 = df_row.bbox1\n","  box1 = Polygon([(x1, y1), (x1, y2), (x2, y2), (x2, y1)])\n","  x1, y1, x2, y2 = df_row.bbox2\n","  box2 = Polygon([(x1, y1), (x1, y2), (x2, y2), (x2, y1)])\n","  df_row['score'] = (float(df_row.conf1) * df_row.weight1 * box1.intersection(box2).area / box1.area) + (float(df_row.conf2) * df_row.weight2 * box1.intersection(box2).area / box2.area)\n","  df_row['score'] += (float(df_row.conf1) * df_row.weight1) + (float(df_row.conf2) * df_row.weight2)\n","  return df_row[['id', 'model1', 'model2', 'score']]\n","  \n","def detect_image(image_path):\n","  preds_list = [[process_pred(model_name, pred) for pred in preds] for (model_name, preds) in get_preds(image_path)]\n","  df = pd.DataFrame([item for sublist in preds_list for item in sublist])\n","  print(df)\n","\n","  if len(df) == 0:\n","    return 0\n","  else:\n","    df = df[df.id >= 11][df.id <= 40][['model', 'bbox', 'conf', 'id']]\n","    crossed_df = pd.concat([df[df.model == model_name].merge(df[df.model != model_name], how = 'outer', on = \"id\") for model_name in models])\n","    crossed_df = crossed_df.apply(lambda x: process_rows(x), axis = 1).reset_index(drop = True)\n","    crossed_df = crossed_df.iloc[crossed_df[['id', 'model1', 'model2', 'conf1', 'conf2']].drop_duplicates().index.to_list()].dropna()\n","    print(crossed_df)\n","    if len(crossed_df) == 0:\n","      return df.sort_values('conf', ascending = False)['id'].iloc[0]\n","    else:\n","      crossed_df = crossed_df.apply(lambda x: calc_score(x), axis = 1)\n","      crossed_df = crossed_df.sort_values(['id', 'model1', 'model2', 'score'], ascending = False).groupby(['id', 'model1', 'model2']).head(1).reset_index(drop = True)\n","      print(crossed_df)\n","      return crossed_df.groupby('id').sum().reset_index(drop = False).sort_values('score', ascending = False)['id'].iloc[0]\n","\n","# detect_image('/content/yolov5/MDPv2-1/test/images/Alpha-S---0034_jpg.rf.592ecad7ee2089f1076cd37c8ad676f0.jpg')\n","# detect_image('/content/yolov5/MDPv2-1/test/images/AlphabetT_light0012_jpg.rf.564c96525f878c1a68c836d1d2b53757.jpg')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TSi1IOKqUNHa"},"outputs":[],"source":["!rm -rf /content/yolov5/MDPv2-1/test/output/*\n","detect_image('/content/yolov5/MDPv2-1/test/images/WhatsApp Image 2022-03-09 at 1.58.48 PM.jpeg')"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"Iif1c9dBzUFP","executionInfo":{"status":"ok","timestamp":1646833747349,"user_tz":-480,"elapsed":454,"user":{"displayName":"Chua Cheng Hong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgB02yYr0OXz-1q3qlmnMPOfkLWcmGM_ckc4HHB=s64","userId":"15038074957184626047"}}},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rIMGfwJDzUH7"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X87cf6xNzUKq"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zin9Fk55zUNH"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y7uXLIC6zUPr"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HY9f0E4TzUSG"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5nrf99opUNJv"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IgUMZHszUNMB"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"04jRWmFIUNOG"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lJKq9WFPY5ok"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xSlmP0VlY5qq"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CwItVersY5s1"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tNP48E_kY5vH"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oUTfMEV1Y5xX"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"MDP_YOLOv5.ipynb","provenance":[{"file_id":"https://github.com/roboflow-ai/yolov5-custom-training-tutorial/blob/main/yolov5-custom-training.ipynb","timestamp":1645938872670}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}